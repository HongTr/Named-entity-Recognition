{"cells":[{"cell_type":"markdown","source":["# Mount drive, import libraries, Hyperparameters"],"metadata":{"id":"p91ygBk6kvEU"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"TvqW18aQCSbk","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1664272506485,"user_tz":-420,"elapsed":3049,"user":{"displayName":"Truong Minh Hong","userId":"16439145051231501400"}},"outputId":"2c541c8f-4c85-42c2-a2f6-e16ac04756c1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","/content/drive/MyDrive/AI\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","%cd '/content/drive/MyDrive/AI'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sSu2z-rGQPzT"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","\n","import torch\n","from torch import nn, optim\n","from torch.nn import functional as F\n","import torch.autograd as autograd\n","torch.manual_seed(1)\n","\n","import pickle\n","\n","from sklearn.model_selection import train_test_split"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"etclMUJrjZPt"},"outputs":[],"source":["device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu') #'cuda:0'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4n1glilH6T02","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1664272510402,"user_tz":-420,"elapsed":3933,"user":{"displayName":"Truong Minh Hong","userId":"16439145051231501400"}},"outputId":"2f36d21b-fdc0-44b1-a191-ba904aaf7f71"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: pyvi in /usr/local/lib/python3.7/dist-packages (0.1.1)\n","Requirement already satisfied: sklearn-crfsuite in /usr/local/lib/python3.7/dist-packages (from pyvi) (0.3.6)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from pyvi) (1.0.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->pyvi) (3.1.0)\n","Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->pyvi) (1.7.3)\n","Requirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->pyvi) (1.21.6)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->pyvi) (1.1.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sklearn-crfsuite->pyvi) (1.15.0)\n","Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from sklearn-crfsuite->pyvi) (0.8.10)\n","Requirement already satisfied: tqdm>=2.0 in /usr/local/lib/python3.7/dist-packages (from sklearn-crfsuite->pyvi) (4.64.1)\n","Requirement already satisfied: python-crfsuite>=0.8.3 in /usr/local/lib/python3.7/dist-packages (from sklearn-crfsuite->pyvi) (0.9.8)\n"]}],"source":["!pip install pyvi\n","from pyvi import ViPosTagger"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Md6tfTejYoTp"},"outputs":[],"source":["EMBEDDING_DIM = 100\n","HIDDEN_DIM = 200\n","CHAR_EMBEDDING_DIM = 100\n","CHAR_HIDDEN_DIM = 10\n","CHAR_TARGET_SIZE = 50\n","\n","EPOCHS = 10\n","BATCH_SIZE = 32\n","LEARNING_RATE = 1e-3"]},{"cell_type":"markdown","metadata":{"id":"smGFqkdxBGBJ"},"source":["# Read Pretrained data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gNE08vnAgzbn"},"outputs":[],"source":["pretrain_data_0 = pd.read_csv('pretrained100.txt', sep=\" \",header = None, skiprows = 1, nrows = 100000)\n","pretrain_data_1 = pd.read_csv('pretrained100.txt', sep=\" \",header = None, skiprows = 100001, nrows = 100000)\n","pretrain_data_2 = pd.read_csv('pretrained100.txt', sep=\" \",header = None, skiprows = 200001, nrows = 100000)\n","pretrain_data_3 = pd.read_csv('pretrained100.txt', sep=\" \",header = None, skiprows = 300001, nrows = 100000)\n","pretrain_data = pd.concat([pretrain_data_0, pretrain_data_1, pretrain_data_2, pretrain_data_3])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":317},"executionInfo":{"elapsed":16,"status":"ok","timestamp":1661956747773,"user":{"displayName":"Hồng Trương Minh","userId":"11412969009276086269"},"user_tz":-420},"id":"6oPugpA6YRya","outputId":"090a1646-63b5-41cf-b50c-97583ae6145c"},"outputs":[{"name":"stdout","output_type":"stream","text":["(400000, 101)\n"]},{"data":{"text/html":["\n","  <div id=\"df-131fcd21-0748-4692-b475-d9d7dc899411\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","      <th>1</th>\n","      <th>2</th>\n","      <th>3</th>\n","      <th>4</th>\n","      <th>5</th>\n","      <th>6</th>\n","      <th>7</th>\n","      <th>8</th>\n","      <th>9</th>\n","      <th>...</th>\n","      <th>91</th>\n","      <th>92</th>\n","      <th>93</th>\n","      <th>94</th>\n","      <th>95</th>\n","      <th>96</th>\n","      <th>97</th>\n","      <th>98</th>\n","      <th>99</th>\n","      <th>100</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>,</td>\n","      <td>-0.013445</td>\n","      <td>0.082912</td>\n","      <td>0.095113</td>\n","      <td>0.052493</td>\n","      <td>0.069441</td>\n","      <td>-0.010651</td>\n","      <td>0.004321</td>\n","      <td>-0.082824</td>\n","      <td>0.051480</td>\n","      <td>...</td>\n","      <td>-0.099876</td>\n","      <td>0.020649</td>\n","      <td>-0.066902</td>\n","      <td>0.077416</td>\n","      <td>-0.147332</td>\n","      <td>-0.021268</td>\n","      <td>0.019100</td>\n","      <td>-0.138095</td>\n","      <td>0.077689</td>\n","      <td>-0.210762</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>.</td>\n","      <td>0.080173</td>\n","      <td>0.048418</td>\n","      <td>0.058027</td>\n","      <td>0.055044</td>\n","      <td>0.023451</td>\n","      <td>-0.000025</td>\n","      <td>0.000599</td>\n","      <td>-0.071703</td>\n","      <td>0.054242</td>\n","      <td>...</td>\n","      <td>-0.183366</td>\n","      <td>0.039830</td>\n","      <td>-0.010186</td>\n","      <td>0.075011</td>\n","      <td>-0.128598</td>\n","      <td>-0.020384</td>\n","      <td>0.029758</td>\n","      <td>-0.172362</td>\n","      <td>0.126670</td>\n","      <td>-0.188354</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>và</td>\n","      <td>0.013720</td>\n","      <td>0.058717</td>\n","      <td>0.110645</td>\n","      <td>0.066075</td>\n","      <td>0.046535</td>\n","      <td>-0.004956</td>\n","      <td>-0.003579</td>\n","      <td>-0.033581</td>\n","      <td>0.043340</td>\n","      <td>...</td>\n","      <td>-0.128276</td>\n","      <td>-0.011903</td>\n","      <td>-0.048510</td>\n","      <td>0.085721</td>\n","      <td>-0.133140</td>\n","      <td>0.005551</td>\n","      <td>0.043090</td>\n","      <td>-0.154915</td>\n","      <td>0.148186</td>\n","      <td>-0.196749</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>của</td>\n","      <td>0.026101</td>\n","      <td>0.033375</td>\n","      <td>0.166609</td>\n","      <td>0.013060</td>\n","      <td>0.017971</td>\n","      <td>0.006579</td>\n","      <td>0.067402</td>\n","      <td>0.002216</td>\n","      <td>0.116908</td>\n","      <td>...</td>\n","      <td>-0.137093</td>\n","      <td>0.036225</td>\n","      <td>-0.026152</td>\n","      <td>0.148379</td>\n","      <td>-0.151303</td>\n","      <td>-0.039304</td>\n","      <td>0.016051</td>\n","      <td>-0.123603</td>\n","      <td>0.107642</td>\n","      <td>-0.222042</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>là</td>\n","      <td>0.031769</td>\n","      <td>0.048661</td>\n","      <td>0.138932</td>\n","      <td>-0.068711</td>\n","      <td>0.049129</td>\n","      <td>-0.014016</td>\n","      <td>0.066129</td>\n","      <td>0.013496</td>\n","      <td>0.028830</td>\n","      <td>...</td>\n","      <td>-0.124612</td>\n","      <td>0.031029</td>\n","      <td>-0.069093</td>\n","      <td>0.085157</td>\n","      <td>-0.106477</td>\n","      <td>-0.006654</td>\n","      <td>-0.006391</td>\n","      <td>-0.161527</td>\n","      <td>0.107603</td>\n","      <td>-0.208509</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows × 101 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-131fcd21-0748-4692-b475-d9d7dc899411')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-131fcd21-0748-4692-b475-d9d7dc899411 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-131fcd21-0748-4692-b475-d9d7dc899411');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["   0         1         2         3         4         5         6         7    \\\n","0    , -0.013445  0.082912  0.095113  0.052493  0.069441 -0.010651  0.004321   \n","1    .  0.080173  0.048418  0.058027  0.055044  0.023451 -0.000025  0.000599   \n","2   và  0.013720  0.058717  0.110645  0.066075  0.046535 -0.004956 -0.003579   \n","3  của  0.026101  0.033375  0.166609  0.013060  0.017971  0.006579  0.067402   \n","4   là  0.031769  0.048661  0.138932 -0.068711  0.049129 -0.014016  0.066129   \n","\n","        8         9    ...       91        92        93        94        95   \\\n","0 -0.082824  0.051480  ... -0.099876  0.020649 -0.066902  0.077416 -0.147332   \n","1 -0.071703  0.054242  ... -0.183366  0.039830 -0.010186  0.075011 -0.128598   \n","2 -0.033581  0.043340  ... -0.128276 -0.011903 -0.048510  0.085721 -0.133140   \n","3  0.002216  0.116908  ... -0.137093  0.036225 -0.026152  0.148379 -0.151303   \n","4  0.013496  0.028830  ... -0.124612  0.031029 -0.069093  0.085157 -0.106477   \n","\n","        96        97        98        99        100  \n","0 -0.021268  0.019100 -0.138095  0.077689 -0.210762  \n","1 -0.020384  0.029758 -0.172362  0.126670 -0.188354  \n","2  0.005551  0.043090 -0.154915  0.148186 -0.196749  \n","3 -0.039304  0.016051 -0.123603  0.107642 -0.222042  \n","4 -0.006654 -0.006391 -0.161527  0.107603 -0.208509  \n","\n","[5 rows x 101 columns]"]},"execution_count":44,"metadata":{},"output_type":"execute_result"}],"source":["print(pretrain_data.shape)\n","pretrain_data.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"APQ4iL6QPCUy"},"outputs":[],"source":["word_embedding_dict = {}\n","border = np.sqrt(3/100)\n","count = 0\n","for word in list_of_words:\n","  if pretrain_data.loc[pretrain_data[0] == word].shape[0] == 0:\n","    word_embedding_dict[word] = np.random.uniform(-border,border, 100).tolist()\n","  else:\n","    temp = []\n","    for x in range(1, 101):\n","      _ = pretrain_data.loc[pretrain_data[0] == word, x].values[0]\n","      temp.append(_)\n","    word_embedding_dict[word] = temp"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":457,"status":"ok","timestamp":1661961966699,"user":{"displayName":"Hồng Trương Minh","userId":"11412969009276086269"},"user_tz":-420},"id":"DZEGxCuGTbhF","outputId":"585c1c33-f826-4b8f-d466-8287f9798288"},"outputs":[{"data":{"text/plain":["7886"]},"execution_count":48,"metadata":{},"output_type":"execute_result"}],"source":["len(word_embedding_dict)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iF_J5T1vBgE5"},"outputs":[],"source":["import pickle"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":1408,"status":"ok","timestamp":1661962047785,"user":{"displayName":"Hồng Trương Minh","userId":"11412969009276086269"},"user_tz":-420},"id":"LvVXYAGDpxu8","outputId":"0a5d4c9d-b9a7-4950-dee3-5548885029b5"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["\"with open('saved_dictionary.pkl', 'rb') as f:\\n    loaded_dict = pickle.load(f)\""]},"execution_count":49,"metadata":{},"output_type":"execute_result"}],"source":["import pickle\n","with open('saved_dictionary.pkl', 'wb') as f:\n","    pickle.dump(word_embedding_dict, f)"]},{"cell_type":"markdown","metadata":{"id":"iNUNpA8YBWG-"},"source":["# Data Preprocessing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VsrxvVLNqgq5"},"outputs":[],"source":["with open('saved_dictionary.pkl', 'rb') as f: #read from a pretrained word-embedding file\n","    loaded_dict = pickle.load(f)"]},{"cell_type":"code","source":["dataset = pd.read_csv('data.csv')[['sent', 'anno']]\n","print(f\"Dataset's shape: {dataset.shape}\")\n","dataset.head(3)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"E18XPZbJ0MkR","executionInfo":{"status":"ok","timestamp":1664272516396,"user_tz":-420,"elapsed":12,"user":{"displayName":"Truong Minh Hong","userId":"16439145051231501400"}},"outputId":"61187c8c-fcb0-49b1-c8f6-2f531cc880df"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Dataset's shape: (10017, 2)\n"]},{"output_type":"execute_result","data":{"text/plain":["                                                sent  \\\n","0  Đồng_thời , bệnh_viện tiếp_tục thực_hiện các b...   \n","1  \" Số bệnh_viện có_thể tiếp_nhận bệnh_nhân bị s...   \n","2  Ngoài_ra , những người tiếp_xúc gián_tiếp ( đã...   \n","\n","                                                anno  \n","0  O O O O O O O O O O O O O O O B-ORGANIZATION I...  \n","1  O O O O O O O B-SYMPTOM_AND_DISEASE I-SYMPTOM_...  \n","2  O O O O O O O O O O O O O O O O O O O O O O O ...  "],"text/html":["\n","  <div id=\"df-08040464-9a52-4548-87bc-5e52f05a4c3d\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>sent</th>\n","      <th>anno</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Đồng_thời , bệnh_viện tiếp_tục thực_hiện các b...</td>\n","      <td>O O O O O O O O O O O O O O O B-ORGANIZATION I...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>\" Số bệnh_viện có_thể tiếp_nhận bệnh_nhân bị s...</td>\n","      <td>O O O O O O O B-SYMPTOM_AND_DISEASE I-SYMPTOM_...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Ngoài_ra , những người tiếp_xúc gián_tiếp ( đã...</td>\n","      <td>O O O O O O O O O O O O O O O O O O O O O O O ...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-08040464-9a52-4548-87bc-5e52f05a4c3d')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-08040464-9a52-4548-87bc-5e52f05a4c3d button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-08040464-9a52-4548-87bc-5e52f05a4c3d');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":38}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6GT52mOoCt9F"},"outputs":[],"source":["word_to_ix = {}\n","tag_to_ix = {}\n","PAD = \"<PAD>\"\n","START_TAG = \"<START>\"\n","STOP_TAG = \"<STOP>\"\n","\n","tag_to_ix[START_TAG] = 0\n","tag_to_ix[STOP_TAG] = 1\n","\n","for sentences, tags in zip(dataset['sent'].tolist(), dataset['anno'].tolist()):\n","  for word in sentences.split():\n","    if word not in word_to_ix.keys():\n","      word_to_ix[word] = len(word_to_ix)\n","  for tag in tags.split():\n","    if tag not in tag_to_ix.keys():\n","      tag_to_ix[tag] = len(tag_to_ix)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15,"status":"ok","timestamp":1664272516402,"user":{"displayName":"Truong Minh Hong","userId":"16439145051231501400"},"user_tz":-420},"id":"clTGqy5VEWsc","outputId":"82645417-d529-4578-ef5f-b1bd10df9d81"},"outputs":[{"output_type":"stream","name":"stdout","text":["{'<START>': 0, '<STOP>': 1, 'O': 2, 'B-ORGANIZATION': 3, 'I-ORGANIZATION': 4, 'B-SYMPTOM_AND_DISEASE': 5, 'I-SYMPTOM_AND_DISEASE': 6, 'B-LOCATION': 7, 'B-DATE': 8, 'B-PATIENT_ID': 9, 'B-AGE': 10, 'B-NAME': 11, 'I-DATE': 12, 'B-JOB': 13, 'I-LOCATION': 14, 'B-TRANSPORTATION': 15, 'B-GENDER': 16, 'I-TRANSPORTATION': 17, 'I-JOB': 18, 'I-NAME': 19, 'I-AGE': 20, 'I-PATIENT_ID': 21}\n"]}],"source":["print(tag_to_ix)"]},{"cell_type":"code","source":["print(f\"Number of distinct words: {len(word_to_ix)}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2w8BUR230dub","executionInfo":{"status":"ok","timestamp":1664272516404,"user_tz":-420,"elapsed":15,"user":{"displayName":"Truong Minh Hong","userId":"16439145051231501400"}},"outputId":"e6bb9c23-5f5e-49ef-931f-3b9528730ac5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of distinct words: 7886\n"]}]},{"cell_type":"code","source":["word2tag = dict()\n","word2tag['word'] = list()\n","word2tag['tag'] = list()\n","\n","for sentences, tags in zip(dataset['sent'].tolist(), dataset['anno'].tolist()):\n","  for word, tag in zip (sentences.split(), tags.split()):\n","    word2tag['word'].append(word)\n","    word2tag['tag'].append(tag)\n","dataset_df = pd.DataFrame(word2tag)"],"metadata":{"id":"2GBJuI4n11ad"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset_df.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mez8fjp22qqc","executionInfo":{"status":"ok","timestamp":1664272516837,"user_tz":-420,"elapsed":24,"user":{"displayName":"Truong Minh Hong","userId":"16439145051231501400"}},"outputId":"1a5017c4-7689-495e-ced9-22f125015dc9"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["        word tag\n","0  Đồng_thời   O\n","1          ,   O\n","2  bệnh_viện   O\n","3   tiếp_tục   O\n","4  thực_hiện   O"],"text/html":["\n","  <div id=\"df-6798e513-bef7-4353-b11f-552393d8b70e\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>word</th>\n","      <th>tag</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Đồng_thời</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>,</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>bệnh_viện</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>tiếp_tục</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>thực_hiện</td>\n","      <td>O</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6798e513-bef7-4353-b11f-552393d8b70e')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-6798e513-bef7-4353-b11f-552393d8b70e button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-6798e513-bef7-4353-b11f-552393d8b70e');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":43}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":623,"status":"ok","timestamp":1664272517440,"user":{"displayName":"Truong Minh Hong","userId":"16439145051231501400"},"user_tz":-420},"id":"09CasBwrBhF8","outputId":"1d92fb79-1d56-44cf-b2c3-b25341991e58"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<matplotlib.axes._subplots.AxesSubplot at 0x7ff95fabc150>"]},"metadata":{},"execution_count":44},{"output_type":"display_data","data":{"text/plain":["<Figure size 720x360 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}],"source":["dataset_df[dataset_df[\"tag\"]!=\"O\"][\"tag\"].value_counts().plot(kind=\"bar\", figsize=(10,5))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":86,"status":"ok","timestamp":1664272517447,"user":{"displayName":"Truong Minh Hong","userId":"16439145051231501400"},"user_tz":-420},"id":"lsiEOxgtxpGO","outputId":"e7d10c05-ace8-4cea-a1c8-11eb007ee115"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'#word_counts = data_df.groupby(\"Sentence #\")[\"Word\"].agg([\"count\"])\\nword_counts_list = [len(sent[0]) for sent in data]\\nword_counts = pd.DataFrame(word_counts_list)\\nprint(f\"Longest sentence in the corpus contains {max(word_counts_list)} words.\")\\nprint(f\"Longest sentence: {\\' \\'.join(data[np.argmax(np.array(word_counts_list))][0])}.\")\\nprint(f\"We can see the average length of sentence in the corpus is about 20 words\\n\")\\nword_counts.hist(bins=50, figsize=(8,6));'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":45}],"source":["'''#word_counts = data_df.groupby(\"Sentence #\")[\"Word\"].agg([\"count\"])\n","word_counts_list = [len(sent[0]) for sent in data]\n","word_counts = pd.DataFrame(word_counts_list)\n","print(f\"Longest sentence in the corpus contains {max(word_counts_list)} words.\")\n","print(f\"Longest sentence: {' '.join(data[np.argmax(np.array(word_counts_list))][0])}.\")\n","print(f\"We can see the average length of sentence in the corpus is about 20 words\\n\")\n","word_counts.hist(bins=50, figsize=(8,6));'''"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":79,"status":"ok","timestamp":1664272517448,"user":{"displayName":"Truong Minh Hong","userId":"16439145051231501400"},"user_tz":-420},"id":"O0clrIuUr3eW","outputId":"31a21607-f062-4f3f-b83a-6a300d4d16a6"},"outputs":[{"output_type":"stream","name":"stdout","text":["Number of unique tag is 21\n","Number of unique word is 7886\n"]}],"source":["print(f\"Number of unique tag is {len(tag_to_ix) - 1}\")\n","print(f\"Number of unique word is {len(word_to_ix)}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GsSweX5Gr5D5"},"outputs":[],"source":["full_text = \"\"\n","\n","for word in word_to_ix.keys():\n","  full_text += word\n","  full_text += \" \"\n","\n","int2char = dict(enumerate(set(full_text)))\n","char2int = {ch: ii for ii, ch in int2char.items()}\n","int2char[len(char2int)] = 'PAD'\n","char2int['PAD'] = len(char2int)\n","int2char[len(char2int)] = 'UNK'\n","char2int['UNK'] = len(char2int)\n","\n","list_of_pos = {\n","    'X': 0,  #<Unk>\n","    'A': 1,\n","    'C': 2,\n","    'E': 3,\n","    'I': 4,\n","    'L': 5,\n","    'M': 6,\n","    'N': 7,\n","    'Nc': 8,\n","    'Ny': 9,\n","    'Np': 10,\n","    'Nu': 11,\n","    'P': 12,\n","    'R': 13,\n","    'S': 14,\n","    'T': 15,\n","    'V': 16,\n","    'F': 17\n","}\n","\n","def one_hot_encode(index, n_labels):\n","    one_hot = [0] * n_labels\n","    one_hot[index] = 1\n","    return one_hot"]},{"cell_type":"code","source":["print(char2int)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LTyjl-qasdz8","executionInfo":{"status":"ok","timestamp":1664272517452,"user_tz":-420,"elapsed":59,"user":{"displayName":"Truong Minh Hong","userId":"16439145051231501400"}},"outputId":"e1d8f3af-2154-4029-a27b-545fbbd966ac"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{'ẳ': 0, '5': 1, '–': 2, 'j': 3, 'g': 4, ':': 5, 'ơ': 6, 'q': 7, 'F': 8, ' ': 9, '2': 10, 'C': 11, '”': 12, ',': 13, '.': 14, 'ụ': 15, 'ẽ': 16, '3': 17, 'Ễ': 18, 'U': 19, '°': 20, 'À': 21, 'a': 22, 'ằ': 23, 'ế': 24, 'J': 25, '?': 26, 'O': 27, 'â': 28, 'ị': 29, 'Ớ': 30, 'ạ': 31, 'ú': 32, 'T': 33, 'ũ': 34, 'ả': 35, 'f': 36, '&': 37, '￼': 38, \"'\": 39, 'ầ': 40, 'ỹ': 41, 'V': 42, 'ò': 43, 'I': 44, 'Ở': 45, 'ẫ': 46, 'Q': 47, 'Ệ': 48, 'ỳ': 49, 'n': 50, '\"': 51, 'h': 52, '_': 53, '*': 54, 'ư': 55, 'ỏ': 56, '…': 57, 'ỷ': 58, '/': 59, '4': 60, 'ỗ': 61, 'S': 62, 'ó': 63, 'é': 64, ';': 65, 'w': 66, 'X': 67, 'ậ': 68, 'ô': 69, 'ù': 70, 'ễ': 71, 'ớ': 72, 'ĩ': 73, 'ý': 74, 'i': 75, 'ừ': 76, 'ữ': 77, 'c': 78, 'B': 79, 'ã': 80, 'L': 81, 'E': 82, 'W': 83, 'ẹ': 84, 'R': 85, 'Đ': 86, 'Ĩ': 87, 'à': 88, '“': 89, 'b': 90, '8': 91, 'r': 92, 't': 93, 'ờ': 94, 'y': 95, 'Ă': 96, 'k': 97, 'ắ': 98, 'í': 99, '1': 100, 'v': 101, 'Y': 102, '(': 103, '7': 104, 'ề': 105, 'z': 106, 'o': 107, 'ẩ': 108, 'ộ': 109, 'á': 110, 'ỡ': 111, 'Á': 112, 'l': 113, 'ở': 114, 'Ấ': 115, 'Ô': 116, 'ủ': 117, '%': 118, 'ợ': 119, 'Z': 120, 'Ứ': 121, 'M': 122, 'P': 123, 'ặ': 124, 'A': 125, 'ỵ': 126, 'x': 127, 'ố': 128, 'đ': 129, 'G': 130, 'ì': 131, 'Ổ': 132, 'H': 133, 'Í': 134, ')': 135, 'ệ': 136, '\\u200b': 137, 'ứ': 138, '6': 139, 'ử': 140, 'p': 141, '0': 142, '-': 143, 'ẵ': 144, 'Â': 145, 'ồ': 146, 'ê': 147, 'ổ': 148, 'Ý': 149, '+': 150, 'ự': 151, 'D': 152, 'e': 153, 'ỉ': 154, 's': 155, '’': 156, 'ọ': 157, 'ă': 158, 'õ': 159, 'ẻ': 160, 'ấ': 161, 'è': 162, '‘': 163, 'K': 164, 'd': 165, 'N': 166, 'Ả': 167, 'm': 168, 'u': 169, '9': 170, 'Ú': 171, 'ể': 172, 'Ư': 173, 'PAD': 174, 'UNK': 175}\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o5kiC7W_ajY5"},"outputs":[],"source":["training_data = []\n","for sent, tag in zip(dataset['sent'].tolist(), dataset['anno'].tolist()):\n","  list_word = sent.split()\n","  embed = []\n","  sequence_pos = []\n","  list_tag = [tag_to_ix[t] for t in tag.split()]\n","  sequence_char = []\n","  for word in list_word:\n","    embed.append(loaded_dict[word])\n","    temp = []\n","    for ch in list(word):\n","      temp.append(char2int[ch])\n","    sequence_char.append(temp)\n","  for pos in ViPosTagger.postagging(sent)[1]:\n","    sequence_pos.append(one_hot_encode(list_of_pos[pos], len(list_of_pos)))\n","\n","  training_data.append((list_word, embed, list_tag, sequence_pos, sequence_char))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":714,"status":"ok","timestamp":1664272532471,"user":{"displayName":"Truong Minh Hong","userId":"16439145051231501400"},"user_tz":-420},"id":"-yaXyYIFdwhb","outputId":"bab5e2fc-60d0-45d8-ec17-ba29ea2c8b0b"},"outputs":[{"output_type":"stream","name":"stdout","text":["['Hôm_qua', ',', 'hai', 'bệnh_nhân', 'Covid', '-', '19', 'cũng', 'tử_vong', ',', 'có', 'bệnh', 'nền', 'suy', 'thận', 'mạn', '.']\n","[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 5, 6, 6, 2]\n","[[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]]\n","[[133, 69, 168, 53, 7, 169, 22], [13], [52, 22, 75], [90, 136, 50, 52, 53, 50, 52, 28, 50], [11, 107, 101, 75, 165], [143], [100, 170], [78, 34, 50, 4], [93, 140, 53, 101, 107, 50, 4], [13], [78, 63], [90, 136, 50, 52], [50, 105, 50], [155, 169, 95], [93, 52, 68, 50], [168, 31, 50], [14]]\n"]}],"source":["print(training_data[10][0])\n","print(training_data[10][2])\n","print(training_data[10][3])\n","print(training_data[10][4])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KD2-YyfjI_XN"},"outputs":[],"source":["train, temp = train_test_split(training_data, test_size=0.1, shuffle=True, random_state = 1)\n","test, valid = train_test_split(temp, test_size = 0.1, shuffle=True, random_state = 1)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20,"status":"ok","timestamp":1664272532472,"user":{"displayName":"Truong Minh Hong","userId":"16439145051231501400"},"user_tz":-420},"id":"85a6WjochJJD","outputId":"9864bfd1-f28c-4d6c-ff34-eab10874f1a7"},"outputs":[{"output_type":"stream","name":"stdout","text":["9015\n","901\n","101\n"]}],"source":["print(len(train))\n","print(len(test))\n","print(len(valid))"]},{"cell_type":"code","source":[],"metadata":{"id":"YNqMN6e63tZQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2q9B5DO2B7no"},"source":["# Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hG5iXG8V7vhc"},"outputs":[],"source":["def argmax(vec):\n","    # return the argmax as a python int\n","    _, idx = torch.max(vec, 1)\n","    return idx.item()\n","\n","def generate_batch(df, size):\n","    sent_list = []\n","    embeds_list = []\n","    tags_list = []\n","    pos_list = []\n","    char_list = []\n","    for item in df:\n","        sent_list.append(item[0])\n","        embeds_list += item[1]\n","        tags_list += item[2]\n","        pos_list += item[3]\n","        char_list += item[4]\n","        if len(sent_list) == size:\n","            yield sent_list, embeds_list, tags_list, pos_list, char_list\n","            sent_list = []\n","            embeds_list = []\n","            tags_list = []\n","            pos_list = []\n","            char_list = []\n","\n","# Compute log sum exp in a numerically stable way for the forward algorithm\n","def log_sum_exp(vec):\n","    max_score = vec[0, argmax(vec)]\n","    max_score_broadcast = max_score.view(1, -1).expand(1, vec.size()[1])\n","    return max_score + torch.log(torch.sum(torch.exp(vec - max_score_broadcast)))"]},{"cell_type":"markdown","source":["# WordEmbedding"],"metadata":{"id":"kv692Uun9Zm0"}},{"cell_type":"code","source":["validation_data = list()\n","validation_tag = list()\n","\n","for example in valid:\n","  for embed in example[1]:\n","    validation_data.append(embed)\n","  validation_tag.extend(example[2])\n","validation_data = torch.tensor(validation_data, dtype = torch.float32)\n","validation_tag = torch.tensor(validation_tag)"],"metadata":{"id":"3sow9K8jDRtJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","from torch import nn, optim\n","from torch.nn import functional as F\n","import torch.autograd as autograd\n","torch.manual_seed(1)"],"metadata":{"id":"-GBxo3lM9cC0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["EMBEDDING_DIM = 100\n","HIDDEN_DIM = 200\n","\n","EPOCHS = 10\n","BATCH_SIZE = 32\n","LEARNING_RATE = 1e-3"],"metadata":{"id":"RCxqUu0I9ijk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class BiLSTM_CRF(nn.Module):\n","\n","    def __init__(self, vocab_size, tag_to_ix, embedding_dim, hidden_dim):\n","        super(BiLSTM_CRF, self).__init__()\n","        self.embedding_dim = embedding_dim\n","        self.hidden_dim = hidden_dim\n","        self.vocab_size = vocab_size\n","        self.tag_to_ix = tag_to_ix\n","        self.tagset_size = len(tag_to_ix)\n","\n","        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2,num_layers=1, bidirectional=True)\n","\n","        # Maps the output of the LSTM into tag space.\n","        self.hidden2tag = nn.Linear(hidden_dim, self.tagset_size)\n","\n","        # Matrix of transition parameters.  Entry i,j is the score of\n","        # transitioning *to* i *from* j.\n","        self.transitions = nn.Parameter(torch.randn(self.tagset_size, self.tagset_size))\n","\n","        # These two statements enforce the constraint that we never transfer\n","        # to the start tag and we never transfer from the stop tag\n","        self.transitions.data[tag_to_ix[START_TAG], :] = -10000\n","        self.transitions.data[:, tag_to_ix[STOP_TAG]] = -10000\n","\n","        self.hidden = self.init_hidden()\n","\n","    def init_hidden(self):\n","        hidden = (torch.randn(2, 1, self.hidden_dim // 2),torch.randn(2, 1, self.hidden_dim // 2))\n","        return hidden\n","\n","    def _forward_alg(self, feats):\n","        # Do the forward algorithm to compute the partition function\n","        init_alphas = torch.full((1, self.tagset_size), -10000.)\n","        # START_TAG has all of the score.\n","        init_alphas[0][self.tag_to_ix[START_TAG]] = 0.\n","\n","        # Wrap in a variable so that we will get automatic backprop\n","        forward_var = init_alphas\n","        forward_var = forward_var\n","        # Iterate through the sentence\n","        for feat in feats:\n","            alphas_t = []  # The forward tensors at this timestep\n","            for next_tag in range(self.tagset_size):\n","                # broadcast the emission score: it is the same regardless of\n","                # the previous tag\n","                emit_score = feat[next_tag].view(1, -1).expand(1, self.tagset_size)\n","                # the ith entry of trans_score is the score of transitioning to\n","                # next_tag from i\n","                trans_score = self.transitions[next_tag].view(1, -1)\n","                # The ith entry of next_tag_var is the value for the\n","                # edge (i -> next_tag) before we do log-sum-exp\n","                next_tag_var = forward_var + trans_score + emit_score\n","                # The forward variable for this tag is log-sum-exp of all the\n","                # scores.\n","                alphas_t.append(log_sum_exp(next_tag_var).view(1))\n","            forward_var = torch.cat(alphas_t).view(1, -1)\n","        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n","        alpha = log_sum_exp(terminal_var)\n","        return alpha\n","\n","    def _get_lstm_features(self, sentence):\n","        self.hidden = self.init_hidden()\n","        embeds = sentence.view(len(sentence), 1, -1)     #self.word_embeds(sentence).view(len(sentence), 1, -1)\n","        lstm_out, self.hidden = self.lstm(embeds, self.hidden)\n","        lstm_out = lstm_out.view(len(sentence), self.hidden_dim)\n","        lstm_feats = self.hidden2tag(lstm_out)\n","        return lstm_feats\n","\n","    def _score_sentence(self, feats, tags):\n","        # Gives the score of a provided tag sequence\n","        score = torch.zeros(1)\n","        tags = torch.cat([torch.tensor([self.tag_to_ix[START_TAG]], dtype=torch.long), tags])\n","        for i, feat in enumerate(feats):\n","            score = score + self.transitions[tags[i + 1], tags[i]] + feat[tags[i + 1]]\n","        score = score + self.transitions[self.tag_to_ix[STOP_TAG], tags[-1]]\n","        return score\n","\n","    def _viterbi_decode(self, feats):\n","        backpointers = []\n","\n","        # Initialize the viterbi variables in log space\n","        init_vvars = torch.full((1, self.tagset_size), -10000.)\n","        init_vvars[0][self.tag_to_ix[START_TAG]] = 0\n","\n","        # forward_var at step i holds the viterbi variables for step i-1\n","        forward_var = init_vvars\n","        forward_var = forward_var\n","        \n","        for feat in feats:\n","            bptrs_t = []  # holds the backpointers for this step\n","            viterbivars_t = []  # holds the viterbi variables for this step\n","\n","            for next_tag in range(self.tagset_size):\n","                # next_tag_var[i] holds the viterbi variable for tag i at the\n","                # previous step, plus the score of transitioning\n","                # from tag i to next_tag.\n","                # We don't include the emission scores here because the max\n","                # does not depend on them (we add them in below)\n","                \n","                next_tag_var = forward_var + self.transitions[next_tag]\n","                best_tag_id = argmax(next_tag_var)\n","                bptrs_t.append(best_tag_id)\n","                viterbivars_t.append(next_tag_var[0][best_tag_id].view(1))\n","            # Now add in the emission scores, and assign forward_var to the set\n","            # of viterbi variables we just computed\n","            forward_var = (torch.cat(viterbivars_t) + feat).view(1, -1)\n","            backpointers.append(bptrs_t)\n","\n","        # Transition to STOP_TAG\n","        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n","        best_tag_id = argmax(terminal_var)\n","        path_score = terminal_var[0][best_tag_id]\n","\n","        # Follow the back pointers to decode the best path.\n","        best_path = [best_tag_id]\n","        for bptrs_t in reversed(backpointers):\n","            best_tag_id = bptrs_t[best_tag_id]\n","            best_path.append(best_tag_id)\n","        # Pop off the start tag (we dont want to return that to the caller)\n","        start = best_path.pop()\n","        assert start == self.tag_to_ix[START_TAG]  # Sanity check\n","        best_path.reverse()\n","        return path_score, best_path\n","\n","    def neg_log_likelihood(self, sentence, tags):\n","        feats = self._get_lstm_features(sentence)\n","        forward_score = self._forward_alg(feats)\n","        gold_score = self._score_sentence(feats, tags)\n","        return forward_score - gold_score\n","\n","    def forward(self, sentence):  # dont confuse this with _forward_alg above.\n","        # Get the emission scores from the BiLSTM\n","        lstm_feats = self._get_lstm_features(sentence)\n","\n","        # Find the best path, given the features.\n","        score, tag_seq = self._viterbi_decode(lstm_feats)\n","        return score, tag_seq\n","\n","\n","model = BiLSTM_CRF(len(word_to_ix), tag_to_ix, EMBEDDING_DIM, HIDDEN_DIM)\n","optimizer = optim.Adam(model.parameters(), lr = LEARNING_RATE)"],"metadata":{"id":"H9SqNATX-tVN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import time\n","\n","patience = 2\n","trigger = 0\n","with torch.no_grad():\n","  last_loss = model.neg_log_likelihood(validation_data, validation_tag)[0].item()\n","\n","for epoch in range(EPOCHS):\n","    print(f\"Epoch: {epoch+1}/{EPOCHS}\")\n","    start_time = time.time()\n","    for index, (sent, embeds, tags, pos, char) in enumerate(generate_batch(train, BATCH_SIZE)):\n","        model.zero_grad()\n","\n","        embeds = torch.tensor(embeds, dtype=torch.float32)\n","        targets = torch.tensor(tags)\n","\n","        train_loss = model.neg_log_likelihood(embeds, targets)\n","        train_loss.backward()\n","        optimizer.step()\n","        \n","    with torch.no_grad():\n","      current_loss = model.neg_log_likelihood(validation_data, validation_tag)[0].item()\n","    if current_loss > last_loss:\n","      trigger += 1\n","    else:\n","      last_loss = current_loss\n","    if trigger == patience:\n","      break\n","    print(f\"epoch {epoch+1}/{EPOCHS} with time: {time.time() - start_time} - validloss: {current_loss}\") \n","    start_time = time.time()"],"metadata":{"id":"IWE1NvBo_Ud3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["torch.save(model.state_dict(), '/content/drive/MyDrive/AI/WordEmbed.pt')\n","with open('test.txt', 'wb') as pickle_file:\n","    pickle.dump(temp, pickle_file)"],"metadata":{"id":"ltl1DEvqVZjb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# WordEmbedding + Char"],"metadata":{"id":"gfjTHSWYWSP9"}},{"cell_type":"code","source":["EMBEDDING_DIM = 150\n","HIDDEN_DIM = 200\n","\n","#char\n","embeddingSize = 10\n","hiddenSize = 25\n","charSize = len(char2int)\n","\n","EPOCHS = 10\n","BATCH_SIZE = 32\n","LEARNING_RATE = 1e-3"],"metadata":{"id":"HclQQrv0WVE3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class CharEmbedding(nn.Module):\n","  def __init__(self, embeddingSize ,hiddenSize, charSize):\n","    super(CharEmbedding, self).__init__()\n","    self.embeddingSize = embeddingSize\n","    self.hiddenSize = hiddenSize\n","    self.embed = nn.Embedding(charSize, embeddingSize)\n","    self.BiLSTM = nn.LSTM(embeddingSize, hiddenSize, batch_first = True, bidirectional=True)\n","\n","  def forward(self, sentence):\n","    embeddingMatrix = torch.empty(size = (len(sentence), self.hiddenSize*2))\n","    hidden, cell = (torch.randn(2, 1, self.hiddenSize),\n","                    torch.randn(2, 1, self.hiddenSize))\n","    for i, word in enumerate(sentence):\n","      for char in word:\n","        embeds = self.embed(torch.tensor(char))\n","        out, (hidden, cell) = self.BiLSTM(embeds.view(1, 1, -1), (hidden, cell))\n","      embeddingMatrix[i] = out\n","    return embeddingMatrix\n","\n","class BiLSTM_CRF(nn.Module):\n","\n","    def __init__(self, vocab_size, tag_to_ix, embedding_dim, hidden_dim):\n","        super(BiLSTM_CRF, self).__init__()\n","        self.embedding_dim = embedding_dim\n","        self.hidden_dim = hidden_dim\n","        self.vocab_size = vocab_size\n","        self.tag_to_ix = tag_to_ix\n","        self.tagset_size = len(tag_to_ix)\n","\n","        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2,num_layers=1, bidirectional=True)\n","\n","        # Maps the output of the LSTM into tag space.\n","        self.hidden2tag = nn.Linear(hidden_dim, self.tagset_size)\n","\n","        # Matrix of transition parameters.  Entry i,j is the score of\n","        # transitioning *to* i *from* j.\n","        self.transitions = nn.Parameter(torch.randn(self.tagset_size, self.tagset_size))\n","\n","        # These two statements enforce the constraint that we never transfer\n","        # to the start tag and we never transfer from the stop tag\n","        self.transitions.data[tag_to_ix[START_TAG], :] = -10000\n","        self.transitions.data[:, tag_to_ix[STOP_TAG]] = -10000\n","\n","        self.hidden = self.init_hidden()\n","\n","    def init_hidden(self):\n","        hidden = (torch.randn(2, 1, self.hidden_dim // 2),torch.randn(2, 1, self.hidden_dim // 2))\n","        return hidden\n","\n","    def _forward_alg(self, feats):\n","        # Do the forward algorithm to compute the partition function\n","        init_alphas = torch.full((1, self.tagset_size), -10000.)\n","        # START_TAG has all of the score.\n","        init_alphas[0][self.tag_to_ix[START_TAG]] = 0.\n","\n","        # Wrap in a variable so that we will get automatic backprop\n","        forward_var = init_alphas\n","        forward_var = forward_var\n","        # Iterate through the sentence\n","        for feat in feats:\n","            alphas_t = []  # The forward tensors at this timestep\n","            for next_tag in range(self.tagset_size):\n","                # broadcast the emission score: it is the same regardless of\n","                # the previous tag\n","                emit_score = feat[next_tag].view(1, -1).expand(1, self.tagset_size)\n","                # the ith entry of trans_score is the score of transitioning to\n","                # next_tag from i\n","                trans_score = self.transitions[next_tag].view(1, -1)\n","                # The ith entry of next_tag_var is the value for the\n","                # edge (i -> next_tag) before we do log-sum-exp\n","                next_tag_var = forward_var + trans_score + emit_score\n","                # The forward variable for this tag is log-sum-exp of all the\n","                # scores.\n","                alphas_t.append(log_sum_exp(next_tag_var).view(1))\n","            forward_var = torch.cat(alphas_t).view(1, -1)\n","        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n","        alpha = log_sum_exp(terminal_var)\n","        return alpha\n","\n","    def _get_lstm_features(self, sentence):\n","        self.hidden = self.init_hidden()\n","        embeds = sentence.view(len(sentence), 1, -1)     #self.word_embeds(sentence).view(len(sentence), 1, -1)\n","        lstm_out, self.hidden = self.lstm(embeds, self.hidden)\n","        lstm_out = lstm_out.view(len(sentence), self.hidden_dim)\n","        lstm_feats = self.hidden2tag(lstm_out)\n","        return lstm_feats\n","\n","    def _score_sentence(self, feats, tags):\n","        # Gives the score of a provided tag sequence\n","        score = torch.zeros(1)\n","        tags = torch.cat([torch.tensor([self.tag_to_ix[START_TAG]], dtype=torch.long), tags])\n","        for i, feat in enumerate(feats):\n","            score = score + self.transitions[tags[i + 1], tags[i]] + feat[tags[i + 1]]\n","        score = score + self.transitions[self.tag_to_ix[STOP_TAG], tags[-1]]\n","        return score\n","\n","    def _viterbi_decode(self, feats):\n","        backpointers = []\n","\n","        # Initialize the viterbi variables in log space\n","        init_vvars = torch.full((1, self.tagset_size), -10000.)\n","        init_vvars[0][self.tag_to_ix[START_TAG]] = 0\n","\n","        # forward_var at step i holds the viterbi variables for step i-1\n","        forward_var = init_vvars\n","        forward_var = forward_var\n","        \n","        for feat in feats:\n","            bptrs_t = []  # holds the backpointers for this step\n","            viterbivars_t = []  # holds the viterbi variables for this step\n","\n","            for next_tag in range(self.tagset_size):\n","                # next_tag_var[i] holds the viterbi variable for tag i at the\n","                # previous step, plus the score of transitioning\n","                # from tag i to next_tag.\n","                # We don't include the emission scores here because the max\n","                # does not depend on them (we add them in below)\n","                \n","                next_tag_var = forward_var + self.transitions[next_tag]\n","                best_tag_id = argmax(next_tag_var)\n","                bptrs_t.append(best_tag_id)\n","                viterbivars_t.append(next_tag_var[0][best_tag_id].view(1))\n","            # Now add in the emission scores, and assign forward_var to the set\n","            # of viterbi variables we just computed\n","            forward_var = (torch.cat(viterbivars_t) + feat).view(1, -1)\n","            backpointers.append(bptrs_t)\n","\n","        # Transition to STOP_TAG\n","        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n","        best_tag_id = argmax(terminal_var)\n","        path_score = terminal_var[0][best_tag_id]\n","\n","        # Follow the back pointers to decode the best path.\n","        best_path = [best_tag_id]\n","        for bptrs_t in reversed(backpointers):\n","            best_tag_id = bptrs_t[best_tag_id]\n","            best_path.append(best_tag_id)\n","        # Pop off the start tag (we dont want to return that to the caller)\n","        start = best_path.pop()\n","        assert start == self.tag_to_ix[START_TAG]  # Sanity check\n","        best_path.reverse()\n","        return path_score, best_path\n","\n","    def neg_log_likelihood(self, sentence, tags):\n","        feats = self._get_lstm_features(sentence)\n","        forward_score = self._forward_alg(feats)\n","        gold_score = self._score_sentence(feats, tags)\n","        return forward_score - gold_score\n","\n","    def forward(self, sentence):  # dont confuse this with _forward_alg above.\n","        # Get the emission scores from the BiLSTM\n","        lstm_feats = self._get_lstm_features(sentence)\n","\n","        # Find the best path, given the features.\n","        score, tag_seq = self._viterbi_decode(lstm_feats)\n","        return score, tag_seq\n","\n","charModel = CharEmbedding(embeddingSize, hiddenSize, charSize)\n","model = BiLSTM_CRF(len(word_to_ix), tag_to_ix, EMBEDDING_DIM, HIDDEN_DIM)\n","\n","optimizer = optim.Adam([\n","                {'params': model.parameters()},\n","                {'params': charModel.parameters(), 'lr': 1e-3}\n","            ], lr=1e-3)"],"metadata":{"id":"pljNpgnxWxJ5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for index, (sent, embeds, tags, pos, char) in enumerate(generate_batch(train, BATCH_SIZE)):\n","  embeds = torch.tensor(embeds, dtype = torch.float32)\n","  char = charModel(char)\n","\n","  inputs = torch.cat((embeds, char), dim=1)\n","  targets = torch.tensor(tags)\n","  #print(embeds.shape)\n","  #print(char.shape)\n","  print(inputs.shape)\n","  print(targets.shape)\n","  print(\"======\")\n","  if index == 5:\n","    break"],"metadata":{"id":"nJUf9Z72NvyA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import time\n","\n","patience = 3\n","trigger = 0\n","\n","list_word = []\n","list_tag = []\n","list_char = []\n","for i in range(len(valid)):\n","    list_char.extend(valid[i][-1])\n","    list_word.extend(valid[i][1])\n","    list_tag.extend(valid[i][2])\n","\n","with torch.no_grad():\n","  out = charModel(list_char)\n","  test = torch.cat((torch.tensor(list_word, dtype = torch.float32), out), dim = 1)\n","  last_loss = model.neg_log_likelihood(test, torch.tensor(list_tag)).item()\n","  print(f\"Validation Loss before training: {last_loss}\")\n","for epoch in range(EPOCHS):\n","    print(f\"Epoch: {epoch+1}/{EPOCHS}\")\n","    start_time = time.time()\n","\n","    for index, (sent, embeds, tags, pos, char) in enumerate(generate_batch(train, BATCH_SIZE)):\n","        model.zero_grad()\n","        charModel.zero_grad()\n","\n","        embeds = torch.tensor(embeds, dtype = torch.float32)\n","        char = charModel(char)\n","        inputs = torch.cat((embeds, char), dim=1)\n","\n","        targets = torch.tensor(tags)\n","\n","        train_loss = model.neg_log_likelihood(inputs, targets)\n","        train_loss.backward()\n","        optimizer.step()\n","      \n","        print(index, end = \" \")\n","        if index == 10:\n","          break\n","        \n","    with torch.no_grad():\n","      out = charModel(list_char)\n","      test = torch.cat((torch.tensor(list_word, dtype = torch.float32), out), dim = 1)\n","      current_loss = model.neg_log_likelihood(test, torch.tensor(list_tag)).item()\n","      print(f'current loss: {current_loss}')\n","\n","    if current_loss > last_loss:\n","      trigger += 1\n","    else:\n","      last_loss = current_loss\n","    if trigger == patience:\n","      break\n","    print(f\"epoch {epoch+1}/{EPOCHS} with time: {time.time() - start_time} - trainloss: {train_loss.item()} - validloss: {current_loss}\") \n","    start_time = time.time()"],"metadata":{"id":"_kXzMgpnW0kp","colab":{"base_uri":"https://localhost:8080/"},"outputId":"6b6620dd-b7cb-43c0-fe6b-9f01ff72b8a6","executionInfo":{"status":"ok","timestamp":1664273728502,"user_tz":-420,"elapsed":1037715,"user":{"displayName":"Truong Minh Hong","userId":"16439145051231501400"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Validation Loss before training: 8897.66015625\n","Epoch: 1/10\n","0 1 2 3 4 5 6 7 8 9 10 current loss: 2896.6845703125\n","epoch 1/10 with time: 102.71579241752625 - trainloss: 913.5830078125 - validloss: 2896.6845703125\n","Epoch: 2/10\n","0 1 2 3 4 5 6 7 8 9 10 current loss: 2677.40087890625\n","epoch 2/10 with time: 103.85851168632507 - trainloss: 840.3251953125 - validloss: 2677.40087890625\n","Epoch: 3/10\n","0 1 2 3 4 5 6 7 8 9 10 current loss: 2403.54296875\n","epoch 3/10 with time: 103.00330638885498 - trainloss: 742.765625 - validloss: 2403.54296875\n","Epoch: 4/10\n","0 1 2 3 4 5 6 7 8 9 10 current loss: 2066.58984375\n","epoch 4/10 with time: 103.35905861854553 - trainloss: 619.808837890625 - validloss: 2066.58984375\n","Epoch: 5/10\n","0 1 2 3 4 5 6 7 8 9 10 current loss: 1655.5966796875\n","epoch 5/10 with time: 101.11388564109802 - trainloss: 467.571533203125 - validloss: 1655.5966796875\n","Epoch: 6/10\n","0 1 2 3 4 5 6 7 8 9 10 current loss: 1378.544921875\n","epoch 6/10 with time: 104.22508263587952 - trainloss: 365.338623046875 - validloss: 1378.544921875\n","Epoch: 7/10\n","0 1 2 3 4 5 6 7 8 9 10 current loss: 1110.98046875\n","epoch 7/10 with time: 100.93099212646484 - trainloss: 281.029296875 - validloss: 1110.98046875\n","Epoch: 8/10\n","0 1 2 3 4 5 6 7 8 9 10 current loss: 956.5927734375\n","epoch 8/10 with time: 104.5723991394043 - trainloss: 211.619140625 - validloss: 956.5927734375\n","Epoch: 9/10\n","0 1 2 3 4 5 6 7 8 9 10 current loss: 849.42578125\n","epoch 9/10 with time: 104.03422021865845 - trainloss: 166.4033203125 - validloss: 849.42578125\n","Epoch: 10/10\n","0 1 2 3 4 5 6 7 8 9 10 current loss: 831.69140625\n","epoch 10/10 with time: 103.37754225730896 - trainloss: 128.3046875 - validloss: 831.69140625\n"]}]},{"cell_type":"markdown","source":["# WordEmbedding + Char + POS"],"metadata":{"id":"ZWJyiZdrWa9l"}},{"cell_type":"code","source":["EMBEDDING_DIM = 168\n","HIDDEN_DIM = 200\n","\n","#char\n","embeddingSize = 10\n","hiddenSize = 25\n","charSize = len(char2int)\n","\n","EPOCHS = 10\n","BATCH_SIZE = 32\n","LEARNING_RATE = 1e-3"],"metadata":{"id":"AIdCBQLWWgIZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class CharEmbedding(nn.Module):\n","  def __init__(self, embeddingSize ,hiddenSize, charSize):\n","    super(CharEmbedding, self).__init__()\n","    self.embeddingSize = embeddingSize\n","    self.hiddenSize = hiddenSize\n","    self.embed = nn.Embedding(charSize, embeddingSize)\n","    self.BiLSTM = nn.LSTM(embeddingSize, hiddenSize, batch_first = True, bidirectional=True)\n","\n","  def forward(self, sentence):\n","    embeddingMatrix = torch.empty(size = (len(sentence), self.hiddenSize*2))\n","    hidden, cell = (torch.randn(2, 1, self.hiddenSize),\n","                    torch.randn(2, 1, self.hiddenSize))\n","    for i, word in enumerate(sentence):\n","      for char in word:\n","        embeds = self.embed(torch.tensor(char))\n","        out, (hidden, cell) = self.BiLSTM(embeds.view(1, 1, -1), (hidden, cell))\n","      embeddingMatrix[i] = out\n","    return embeddingMatrix\n","\n","class BiLSTM_CRF(nn.Module):\n","\n","    def __init__(self, vocab_size, tag_to_ix, embedding_dim, hidden_dim):\n","        super(BiLSTM_CRF, self).__init__()\n","        self.embedding_dim = embedding_dim\n","        self.hidden_dim = hidden_dim\n","        self.vocab_size = vocab_size\n","        self.tag_to_ix = tag_to_ix\n","        self.tagset_size = len(tag_to_ix)\n","\n","        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2,num_layers=1, bidirectional=True)\n","\n","        # Maps the output of the LSTM into tag space.\n","        self.hidden2tag = nn.Linear(hidden_dim, self.tagset_size)\n","\n","        # Matrix of transition parameters.  Entry i,j is the score of\n","        # transitioning *to* i *from* j.\n","        self.transitions = nn.Parameter(torch.randn(self.tagset_size, self.tagset_size))\n","\n","        # These two statements enforce the constraint that we never transfer\n","        # to the start tag and we never transfer from the stop tag\n","        self.transitions.data[tag_to_ix[START_TAG], :] = -10000\n","        self.transitions.data[:, tag_to_ix[STOP_TAG]] = -10000\n","\n","        self.hidden = self.init_hidden()\n","\n","    def init_hidden(self):\n","        hidden = (torch.randn(2, 1, self.hidden_dim // 2),torch.randn(2, 1, self.hidden_dim // 2))\n","        return hidden\n","\n","    def _forward_alg(self, feats):\n","        # Do the forward algorithm to compute the partition function\n","        init_alphas = torch.full((1, self.tagset_size), -10000.)\n","        # START_TAG has all of the score.\n","        init_alphas[0][self.tag_to_ix[START_TAG]] = 0.\n","\n","        # Wrap in a variable so that we will get automatic backprop\n","        forward_var = init_alphas\n","        forward_var = forward_var\n","        # Iterate through the sentence\n","        for feat in feats:\n","            alphas_t = []  # The forward tensors at this timestep\n","            for next_tag in range(self.tagset_size):\n","                # broadcast the emission score: it is the same regardless of\n","                # the previous tag\n","                emit_score = feat[next_tag].view(1, -1).expand(1, self.tagset_size)\n","                # the ith entry of trans_score is the score of transitioning to\n","                # next_tag from i\n","                trans_score = self.transitions[next_tag].view(1, -1)\n","                # The ith entry of next_tag_var is the value for the\n","                # edge (i -> next_tag) before we do log-sum-exp\n","                next_tag_var = forward_var + trans_score + emit_score\n","                # The forward variable for this tag is log-sum-exp of all the\n","                # scores.\n","                alphas_t.append(log_sum_exp(next_tag_var).view(1))\n","            forward_var = torch.cat(alphas_t).view(1, -1)\n","        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n","        alpha = log_sum_exp(terminal_var)\n","        return alpha\n","\n","    def _get_lstm_features(self, sentence):\n","        self.hidden = self.init_hidden()\n","        embeds = sentence.view(len(sentence), 1, -1)     #self.word_embeds(sentence).view(len(sentence), 1, -1)\n","        lstm_out, self.hidden = self.lstm(embeds, self.hidden)\n","        lstm_out = lstm_out.view(len(sentence), self.hidden_dim)\n","        lstm_feats = self.hidden2tag(lstm_out)\n","        return lstm_feats\n","\n","    def _score_sentence(self, feats, tags):\n","        # Gives the score of a provided tag sequence\n","        score = torch.zeros(1)\n","        tags = torch.cat([torch.tensor([self.tag_to_ix[START_TAG]], dtype=torch.long), tags])\n","        for i, feat in enumerate(feats):\n","            score = score + self.transitions[tags[i + 1], tags[i]] + feat[tags[i + 1]]\n","        score = score + self.transitions[self.tag_to_ix[STOP_TAG], tags[-1]]\n","        return score\n","\n","    def _viterbi_decode(self, feats):\n","        backpointers = []\n","\n","        # Initialize the viterbi variables in log space\n","        init_vvars = torch.full((1, self.tagset_size), -10000.)\n","        init_vvars[0][self.tag_to_ix[START_TAG]] = 0\n","\n","        # forward_var at step i holds the viterbi variables for step i-1\n","        forward_var = init_vvars\n","        forward_var = forward_var\n","        \n","        for feat in feats:\n","            bptrs_t = []  # holds the backpointers for this step\n","            viterbivars_t = []  # holds the viterbi variables for this step\n","\n","            for next_tag in range(self.tagset_size):\n","                # next_tag_var[i] holds the viterbi variable for tag i at the\n","                # previous step, plus the score of transitioning\n","                # from tag i to next_tag.\n","                # We don't include the emission scores here because the max\n","                # does not depend on them (we add them in below)\n","                \n","                next_tag_var = forward_var + self.transitions[next_tag]\n","                best_tag_id = argmax(next_tag_var)\n","                bptrs_t.append(best_tag_id)\n","                viterbivars_t.append(next_tag_var[0][best_tag_id].view(1))\n","            # Now add in the emission scores, and assign forward_var to the set\n","            # of viterbi variables we just computed\n","            forward_var = (torch.cat(viterbivars_t) + feat).view(1, -1)\n","            backpointers.append(bptrs_t)\n","\n","        # Transition to STOP_TAG\n","        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n","        best_tag_id = argmax(terminal_var)\n","        path_score = terminal_var[0][best_tag_id]\n","\n","        # Follow the back pointers to decode the best path.\n","        best_path = [best_tag_id]\n","        for bptrs_t in reversed(backpointers):\n","            best_tag_id = bptrs_t[best_tag_id]\n","            best_path.append(best_tag_id)\n","        # Pop off the start tag (we dont want to return that to the caller)\n","        start = best_path.pop()\n","        assert start == self.tag_to_ix[START_TAG]  # Sanity check\n","        best_path.reverse()\n","        return path_score, best_path\n","\n","    def neg_log_likelihood(self, sentence, tags):\n","        feats = self._get_lstm_features(sentence)\n","        forward_score = self._forward_alg(feats)\n","        gold_score = self._score_sentence(feats, tags)\n","        return forward_score - gold_score\n","\n","    def forward(self, sentence):  # dont confuse this with _forward_alg above.\n","        # Get the emission scores from the BiLSTM\n","        lstm_feats = self._get_lstm_features(sentence)\n","\n","        # Find the best path, given the features.\n","        score, tag_seq = self._viterbi_decode(lstm_feats)\n","        return score, tag_seq\n","\n","charModel = CharEmbedding(embeddingSize, hiddenSize, charSize)\n","model = BiLSTM_CRF(len(word_to_ix), tag_to_ix, EMBEDDING_DIM, HIDDEN_DIM)\n","#optimizer = optim.Adam(model.parameters(), lr = LEARNING_RATE)\n","\n","\n","optimizer = optim.Adam([\n","                {'params': model.parameters()},\n","                {'params': charModel.parameters(), 'lr': 1e-3}\n","            ], lr=1e-3)"],"metadata":{"id":"efdvjXlH2n0I"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import time\n","\n","patience = 3\n","trigger = 0\n","\n","list_word = []\n","list_tag = []\n","list_char = []\n","for i in range(len(valid)):\n","    list_char.extend(valid[i][-1])\n","    list_word.extend(valid[i][1])\n","    list_tag.extend(valid[i][2])\n","\n","with torch.no_grad():\n","  out = charModel(list_char)\n","  test = torch.cat((torch.tensor(list_word, dtype = torch.float32), out), dim = 1)\n","  last_loss = model.neg_log_likelihood(test, torch.tensor(list_tag)).item()\n","  print(f\"Validation Loss before training: {last_loss}\")\n","for epoch in range(EPOCHS):\n","    print(f\"Epoch: {epoch+1}/{EPOCHS}\")\n","    start_time = time.time()\n","\n","    for index, (sent, embeds, tags, pos, char) in enumerate(generate_batch(train, BATCH_SIZE)):\n","        model.zero_grad()\n","        charModel.zero_grad()\n","\n","        embeds = torch.tensor(embeds, dtype = torch.float32)\n","        char = charModel(char)\n","        inputs = torch.cat((embeds, char), dim=1)\n","\n","        targets = torch.tensor(tags)\n","\n","        train_loss = model.neg_log_likelihood(inputs, targets)\n","        train_loss.backward()\n","        optimizer.step()\n","      \n","        print(index, end = \" \")\n","        if index == 10:\n","          break\n","        \n","    with torch.no_grad():\n","      out = charModel(list_char)\n","      test = torch.cat((torch.tensor(list_word, dtype = torch.float32), out), dim = 1)\n","      current_loss = model.neg_log_likelihood(test, torch.tensor(list_tag)).item()\n","      print(f'current loss: {current_loss}')\n","\n","    if current_loss > last_loss:\n","      trigger += 1\n","    else:\n","      last_loss = current_loss\n","    if trigger == patience:\n","      break\n","    print(f\"epoch {epoch+1}/{EPOCHS} with time: {time.time() - start_time} - trainloss: {train_loss.item()} - validloss: {current_loss}\") \n","    start_time = time.time()"],"metadata":{"id":"KXWAfUz12ycV"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"collapsed_sections":["p91ygBk6kvEU","smGFqkdxBGBJ","iNUNpA8YBWG-","2q9B5DO2B7no","kv692Uun9Zm0"],"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}